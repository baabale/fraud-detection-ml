{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis for Fraud Detection\n",
    "\n",
    "This notebook explores the financial transactions dataset to understand patterns, distributions, and relationships that might be relevant for fraud detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Dataset\n",
    "\n",
    "First, we'll load the raw transaction data from the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define the path to the raw data file\n",
    "data_path = '../data/raw/transactions.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(data_path):\n",
    "    # Load the data\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Dataset loaded with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "else:\n",
    "    print(f\"File not found: {data_path}\")\n",
    "    print(\"Please download the dataset from Kaggle and place it in the data/raw directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Overview\n",
    "\n",
    "Let's examine the structure and content of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check data types and missing values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Summary statistics\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (missing_values / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Values': missing_values,\n",
    "    'Percentage': missing_percent\n",
    "})\n",
    "\n",
    "missing_df[missing_df['Missing Values'] > 0].sort_values('Percentage', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fraud Distribution Analysis\n",
    "\n",
    "Let's examine the distribution of fraudulent vs. legitimate transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check the distribution of the target variable (assuming it's called 'is_fraud')\n",
    "if 'is_fraud' in df.columns:\n",
    "    fraud_distribution = df['is_fraud'].value_counts(normalize=True) * 100\n",
    "    print(\"Fraud distribution:\")\n",
    "    print(f\"Legitimate transactions: {fraud_distribution[0]:.2f}%\")\n",
    "    print(f\"Fraudulent transactions: {fraud_distribution[1]:.2f}%\")\n",
    "    \n",
    "    # Plot the distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(x='is_fraud', data=df)\n",
    "    plt.title('Distribution of Fraudulent vs. Legitimate Transactions')\n",
    "    plt.xlabel('Is Fraud')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks([0, 1], ['Legitimate', 'Fraudulent'])\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Target variable 'is_fraud' not found in the dataset.\")\n",
    "    print(\"Available columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transaction Amount Analysis\n",
    "\n",
    "Let's analyze the transaction amounts and how they differ between fraudulent and legitimate transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check if 'amount' column exists\n",
    "if 'amount' in df.columns and 'is_fraud' in df.columns:\n",
    "    # Basic statistics of transaction amounts by fraud status\n",
    "    amount_stats = df.groupby('is_fraud')['amount'].describe()\n",
    "    print(\"Transaction amount statistics by fraud status:\")\n",
    "    print(amount_stats)\n",
    "    \n",
    "    # Plot distribution of transaction amounts\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Use log scale for better visualization\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(data=df, x='amount', hue='is_fraud', bins=50, alpha=0.7)\n",
    "    plt.title('Distribution of Transaction Amounts')\n",
    "    plt.xlabel('Amount')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(['Legitimate', 'Fraudulent'])\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(data=df, x='amount', hue='is_fraud', bins=50, alpha=0.7, log_scale=True)\n",
    "    plt.title('Distribution of Transaction Amounts (Log Scale)')\n",
    "    plt.xlabel('Amount (Log Scale)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(['Legitimate', 'Fraudulent'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Box plot of amounts by fraud status\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='is_fraud', y='amount', data=df)\n",
    "    plt.title('Transaction Amounts by Fraud Status')\n",
    "    plt.xlabel('Is Fraud')\n",
    "    plt.ylabel('Amount')\n",
    "    plt.xticks([0, 1], ['Legitimate', 'Fraudulent'])\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Required columns ('amount' or 'is_fraud') not found in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Temporal Analysis\n",
    "\n",
    "Let's analyze the temporal patterns of transactions and fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check if 'timestamp' column exists\n",
    "if 'timestamp' in df.columns:\n",
    "    # Convert timestamp to datetime if it's not already\n",
    "    if df['timestamp'].dtype != 'datetime64[ns]':\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Extract time components\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['day'] = df['timestamp'].dt.day\n",
    "    df['dayofweek'] = df['timestamp'].dt.dayofweek\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "    \n",
    "    # Plot transactions by hour of day\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    if 'is_fraud' in df.columns:\n",
    "        # Group by hour and fraud status\n",
    "        hour_fraud = df.groupby(['hour', 'is_fraud']).size().unstack(fill_value=0)\n",
    "        hour_fraud.plot(kind='bar', stacked=True)\n",
    "        plt.title('Transactions by Hour of Day and Fraud Status')\n",
    "        plt.xlabel('Hour of Day')\n",
    "        plt.ylabel('Number of Transactions')\n",
    "        plt.legend(['Legitimate', 'Fraudulent'])\n",
    "    else:\n",
    "        # Group by hour only\n",
    "        hour_counts = df['hour'].value_counts().sort_index()\n",
    "        hour_counts.plot(kind='bar')\n",
    "        plt.title('Transactions by Hour of Day')\n",
    "        plt.xlabel('Hour of Day')\n",
    "        plt.ylabel('Number of Transactions')\n",
    "    \n",
    "    plt.xticks(rotation=0)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot transactions by day of week\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    if 'is_fraud' in df.columns:\n",
    "        # Group by day of week and fraud status\n",
    "        day_fraud = df.groupby(['dayofweek', 'is_fraud']).size().unstack(fill_value=0)\n",
    "        day_fraud.plot(kind='bar', stacked=True)\n",
    "        plt.title('Transactions by Day of Week and Fraud Status')\n",
    "        plt.xlabel('Day of Week (0=Monday, 6=Sunday)')\n",
    "        plt.ylabel('Number of Transactions')\n",
    "        plt.legend(['Legitimate', 'Fraudulent'])\n",
    "    else:\n",
    "        # Group by day of week only\n",
    "        day_counts = df['dayofweek'].value_counts().sort_index()\n",
    "        day_counts.plot(kind='bar')\n",
    "        plt.title('Transactions by Day of Week')\n",
    "        plt.xlabel('Day of Week (0=Monday, 6=Sunday)')\n",
    "        plt.ylabel('Number of Transactions')\n",
    "    \n",
    "    plt.xticks(rotation=0)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Timestamp column not found in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Categorical Features Analysis\n",
    "\n",
    "Let's analyze categorical features such as merchant category, payment method, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "\n",
    "# For each categorical column, analyze its distribution and relationship with fraud\n",
    "for col in categorical_cols[:5]:  # Limit to first 5 to avoid too many plots\n",
    "    print(f\"\\nAnalyzing column: {col}\")\n",
    "    \n",
    "    # Get value counts\n",
    "    value_counts = df[col].value_counts().head(10)  # Top 10 values\n",
    "    print(f\"Top 10 values:\\n{value_counts}\")\n",
    "    \n",
    "    # Plot distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    if 'is_fraud' in df.columns:\n",
    "        # Cross-tabulation with fraud status\n",
    "        cross_tab = pd.crosstab(df[col], df['is_fraud'])\n",
    "        # Calculate fraud rate for each category\n",
    "        fraud_rate = cross_tab[1] / (cross_tab[0] + cross_tab[1]) * 100\n",
    "        fraud_rate = fraud_rate.sort_values(ascending=False).head(10)\n",
    "        \n",
    "        # Plot fraud rate by category\n",
    "        fraud_rate.plot(kind='bar')\n",
    "        plt.title(f'Fraud Rate by {col} (Top 10)')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Fraud Rate (%)')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "    else:\n",
    "        # Simple value counts\n",
    "        value_counts.plot(kind='bar')\n",
    "        plt.title(f'Distribution of {col} (Top 10)')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Correlation Analysis\n",
    "\n",
    "Let's examine correlations between numeric features and with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Select numeric columns\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "print(f\"Numeric columns: {numeric_cols}\")\n",
    "\n",
    "# Calculate correlation matrix\n",
    "if len(numeric_cols) > 0:\n",
    "    corr_matrix = df[numeric_cols].corr()\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "    plt.title('Correlation Matrix of Numeric Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # If target variable exists, show correlation with it\n",
    "    if 'is_fraud' in numeric_cols:\n",
    "        target_corr = corr_matrix['is_fraud'].sort_values(ascending=False)\n",
    "        print(\"\\nCorrelation with target variable (is_fraud):\")\n",
    "        print(target_corr)\n",
    "        \n",
    "        # Plot top correlations with target\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        target_corr[1:11].plot(kind='bar')  # Exclude self-correlation and show top 10\n",
    "        plt.title('Top 10 Features Correlated with Fraud')\n",
    "        plt.xlabel('Feature')\n",
    "        plt.ylabel('Correlation Coefficient')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No numeric columns found in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Engineering Ideas\n",
    "\n",
    "Based on the exploratory analysis, here are some feature engineering ideas that might be useful for fraud detection:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Time-based Features\n",
    "\n",
    "- Hour of day, day of week, month\n",
    "- Is weekend/weekday\n",
    "- Time since last transaction (for the same user)\n",
    "- Transaction velocity (number of transactions in last 1h, 24h, 7d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Amount-based Features\n",
    "\n",
    "- Log-transformed amount\n",
    "- Amount bins/quantiles\n",
    "- Deviation from user's average transaction amount\n",
    "- Ratio to maximum previous transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Behavioral Features\n",
    "\n",
    "- User's transaction history statistics\n",
    "- Merchant-specific patterns\n",
    "- Location-based features (new location, distance from usual locations)\n",
    "- Device-related features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Next Steps\n",
    "\n",
    "Based on this exploratory analysis, the next steps would be:\n",
    "\n",
    "1. Data preprocessing:\n",
    "   - Handle missing values\n",
    "   - Convert categorical features to numeric (encoding)\n",
    "   - Scale/normalize numeric features\n",
    "\n",
    "2. Feature engineering:\n",
    "   - Implement the feature ideas identified above\n",
    "   - Create aggregated features by user, merchant, etc.\n",
    "\n",
    "3. Model development:\n",
    "   - Split data into train/validation/test sets\n",
    "   - Train classification and anomaly detection models\n",
    "   - Evaluate and compare model performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
