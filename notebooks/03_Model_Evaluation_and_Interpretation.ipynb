{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Interpretation\n",
    "\n",
    "This notebook evaluates the trained fraud detection models and interprets their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_curve, roc_auc_score, precision_recall_curve, average_precision_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "import tensorflow as tf\n",
    "\n",
    "# Add the src directory to the path to import our modules\n",
    "sys.path.append('..')\n",
    "from src.models.fraud_model import compute_anomaly_scores\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Test Data and Models\n",
    "\n",
    "First, we'll load the test data and the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load test data\n",
    "test_data_path = '../data/processed/test_data.parquet'\n",
    "if os.path.exists(test_data_path):\n",
    "    test_df = pd.read_parquet(test_data_path)\n",
    "    print(f\"Test data loaded with {test_df.shape[0]} samples and {test_df.shape[1]} columns\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    y_test = test_df['is_fraud'].values\n",
    "    X_test = test_df.drop(columns=['is_fraud']).values\n",
    "else:\n",
    "    print(f\"Test data not found: {test_data_path}\")\n",
    "    print(\"Please run the model training notebook first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load models\n",
    "model_dir = '../results/models'\n",
    "classification_model_path = os.path.join(model_dir, 'classification_model.h5')\n",
    "autoencoder_model_path = os.path.join(model_dir, 'autoencoder_model.h5')\n",
    "\n",
    "# Load classification model\n",
    "if os.path.exists(classification_model_path):\n",
    "    classification_model = tf.keras.models.load_model(classification_model_path)\n",
    "    print(\"Classification model loaded successfully.\")\n",
    "else:\n",
    "    print(f\"Classification model not found: {classification_model_path}\")\n",
    "\n",
    "# Load autoencoder model\n",
    "if os.path.exists(autoencoder_model_path):\n",
    "    autoencoder_model = tf.keras.models.load_model(autoencoder_model_path)\n",
    "    print(\"Autoencoder model loaded successfully.\")\n",
    "else:\n",
    "    print(f\"Autoencoder model not found: {autoencoder_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluate Classification Model\n",
    "\n",
    "Let's evaluate the performance of the classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get predictions from classification model\n",
    "if 'classification_model' in locals():\n",
    "    y_pred_proba = classification_model.predict(X_test).flatten()\n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"Classification Model Performance:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix - Classification Model')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {auc:.4f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve - Classification Model')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot Precision-Recall curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    ap = average_precision_score(y_test, y_pred_proba)\n",
    "    plt.plot(recall_curve, precision_curve, label=f'AP = {ap:.4f}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve - Classification Model')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate Autoencoder Model\n",
    "\n",
    "Now, let's evaluate the performance of the autoencoder model for anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get anomaly scores from autoencoder model\n",
    "if 'autoencoder_model' in locals():\n",
    "    # Compute reconstruction error (anomaly scores)\n",
    "    anomaly_scores = compute_anomaly_scores(autoencoder_model, X_test)\n",
    "    \n",
    "    # Determine threshold (95th percentile of non-fraud scores)\n",
    "    non_fraud_indices = (y_test == 0)\n",
    "    non_fraud_scores = anomaly_scores[non_fraud_indices]\n",
    "    threshold = np.percentile(non_fraud_scores, 95)\n",
    "    print(f\"Anomaly threshold (95th percentile): {threshold:.4f}\")\n",
    "    \n",
    "    # Classify based on threshold\n",
    "    y_pred_autoencoder = (anomaly_scores >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred_autoencoder)\n",
    "    precision = precision_score(y_test, y_pred_autoencoder)\n",
    "    recall = recall_score(y_test, y_pred_autoencoder)\n",
    "    f1 = f1_score(y_test, y_pred_autoencoder)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"Autoencoder Model Performance:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_autoencoder))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_test, y_pred_autoencoder)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix - Autoencoder Model')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot anomaly score distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(anomaly_scores[y_test == 0], label='Normal', alpha=0.5, kde=True)\n",
    "    sns.histplot(anomaly_scores[y_test == 1], label='Fraud', alpha=0.5, kde=True)\n",
    "    plt.axvline(threshold, color='red', linestyle='--', label=f'Threshold: {threshold:.4f}')\n",
    "    plt.xlabel('Anomaly Score (Reconstruction Error)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Anomaly Score Distribution - Autoencoder Model')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot ROC curve for anomaly scores\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, anomaly_scores)\n",
    "    auc = roc_auc_score(y_test, anomaly_scores)\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {auc:.4f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve - Autoencoder Model')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Models\n",
    "\n",
    "Let's compare the performance of both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare model performance if both models are available\n",
    "if 'classification_model' in locals() and 'autoencoder_model' in locals():\n",
    "    # Prepare metrics for comparison\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    classification_metrics = [\n",
    "        accuracy_score(y_test, y_pred),\n",
    "        precision_score(y_test, y_pred),\n",
    "        recall_score(y_test, y_pred),\n",
    "        f1_score(y_test, y_pred)\n",
    "    ]\n",
    "    autoencoder_metrics = [\n",
    "        accuracy_score(y_test, y_pred_autoencoder),\n",
    "        precision_score(y_test, y_pred_autoencoder),\n",
    "        recall_score(y_test, y_pred_autoencoder),\n",
    "        f1_score(y_test, y_pred_autoencoder)\n",
    "    ]\n",
    "    \n",
    "    # Create comparison dataframe\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Metric': metrics,\n",
    "        'Classification Model': classification_metrics,\n",
    "        'Autoencoder Model': autoencoder_metrics\n",
    "    })\n",
    "    \n",
    "    # Display comparison\n",
    "    print(\"Model Performance Comparison:\")\n",
    "    print(comparison_df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    comparison_df.set_index('Metric').plot(kind='bar')\n",
    "    plt.title('Model Performance Comparison')\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(title='Model')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Analysis\n",
    "\n",
    "Let's analyze which features are most important for fraud detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze feature importance for classification model\n",
    "if 'classification_model' in locals():\n",
    "    # Get feature names\n",
    "    feature_names = test_df.drop(columns=['is_fraud']).columns.tolist()\n",
    "    \n",
    "    # For a simple model, we can use permutation importance\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    \n",
    "    # Define a wrapper function for the TensorFlow model\n",
    "    def model_predict(X):\n",
    "        return classification_model.predict(X).flatten()\n",
    "    \n",
    "    # Calculate permutation importance\n",
    "    result = permutation_importance(\n",
    "        model_predict, X_test, y_test,\n",
    "        n_repeats=10,\n",
    "        random_state=42,\n",
    "        scoring='roc_auc'\n",
    "    )\n",
    "    \n",
    "    # Create importance dataframe\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': result.importances_mean,\n",
    "        'Std': result.importances_std\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Display top 20 features\n",
    "    print(\"Top 20 Important Features:\")\n",
    "    print(importance_df.head(20).to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df.head(20))\n",
    "    plt.title('Feature Importance (Permutation Importance)')\n",
    "    plt.xlabel('Importance (Mean Decrease in AUC)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Threshold Analysis\n",
    "\n",
    "Let's analyze how different threshold values affect model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze classification threshold for classification model\n",
    "if 'classification_model' in locals():\n",
    "    # Define threshold range\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)\n",
    "    \n",
    "    # Calculate metrics for each threshold\n",
    "    threshold_metrics = []\n",
    "    for threshold in thresholds:\n",
    "        y_pred_threshold = (y_pred_proba >= threshold).astype(int)\n",
    "        threshold_metrics.append({\n",
    "            'Threshold': threshold,\n",
    "            'Accuracy': accuracy_score(y_test, y_pred_threshold),\n",
    "            'Precision': precision_score(y_test, y_pred_threshold),\n",
    "            'Recall': recall_score(y_test, y_pred_threshold),\n",
    "            'F1 Score': f1_score(y_test, y_pred_threshold)\n",
    "        })\n",
    "    \n",
    "    # Create threshold dataframe\n",
    "    threshold_df = pd.DataFrame(threshold_metrics)\n",
    "    \n",
    "    # Display threshold metrics\n",
    "    print(\"Metrics at Different Thresholds:\")\n",
    "    print(threshold_df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "    \n",
    "    # Plot threshold metrics\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for metric in ['Accuracy', 'Precision', 'Recall', 'F1 Score']:\n",
    "        plt.plot(threshold_df['Threshold'], threshold_df[metric], marker='o', label=metric)\n",
    "    plt.title('Metrics vs. Classification Threshold')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Analysis\n",
    "\n",
    "Let's analyze the errors made by the models to understand where they fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze errors made by the classification model\n",
    "if 'classification_model' in locals():\n",
    "    # Create a dataframe with actual and predicted values\n",
    "    error_df = test_df.copy()\n",
    "    error_df['predicted_proba'] = y_pred_proba\n",
    "    error_df['predicted'] = y_pred\n",
    "    error_df['correct'] = (error_df['is_fraud'] == error_df['predicted'])\n",
    "    \n",
    "    # Separate into different error types\n",
    "    false_positives = error_df[(error_df['is_fraud'] == 0) & (error_df['predicted'] == 1)]\n",
    "    false_negatives = error_df[(error_df['is_fraud'] == 1) & (error_df['predicted'] == 0)]\n",
    "    \n",
    "    print(f\"Number of false positives: {len(false_positives)}\")\n",
    "    print(f\"Number of false negatives: {len(false_negatives)}\")\n",
    "    \n",
    "    # Analyze false positives\n",
    "    if len(false_positives) > 0:\n",
    "        print(\"\\nFalse Positive Analysis:\")\n",
    "        print(\"Sample of false positives (legitimate transactions classified as fraud):\")\n",
    "        print(false_positives.head(5))\n",
    "        \n",
    "        # Analyze feature distributions for false positives\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for i, feature in enumerate(feature_names[:5]):  # Analyze first 5 features\n",
    "            plt.subplot(2, 3, i+1)\n",
    "            sns.histplot(error_df[error_df['is_fraud'] == 0][feature], label='Normal', alpha=0.5, kde=True)\n",
    "            sns.histplot(false_positives[feature], label='False Positive', alpha=0.5, kde=True)\n",
    "            plt.title(f'{feature} Distribution')\n",
    "            plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Analyze false negatives\n",
    "    if len(false_negatives) > 0:\n",
    "        print(\"\\nFalse Negative Analysis:\")\n",
    "        print(\"Sample of false negatives (fraudulent transactions classified as legitimate):\")\n",
    "        print(false_negatives.head(5))\n",
    "        \n",
    "        # Analyze feature distributions for false negatives\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for i, feature in enumerate(feature_names[:5]):  # Analyze first 5 features\n",
    "            plt.subplot(2, 3, i+1)\n",
    "            sns.histplot(error_df[error_df['is_fraud'] == 1][feature], label='Fraud', alpha=0.5, kde=True)\n",
    "            sns.histplot(false_negatives[feature], label='False Negative', alpha=0.5, kde=True)\n",
    "            plt.title(f'{feature} Distribution')\n",
    "            plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "In this notebook, we've:\n",
    "1. Evaluated the performance of both classification and autoencoder models\n",
    "2. Compared the models using various metrics\n",
    "3. Analyzed feature importance to understand which features are most predictive of fraud\n",
    "4. Performed threshold analysis to find the optimal classification threshold\n",
    "5. Analyzed errors made by the models to understand their limitations\n",
    "\n",
    "These insights can help improve the models and guide deployment decisions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
