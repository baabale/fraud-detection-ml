{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Hyperparameter Tuning\n",
    "\n",
    "This notebook demonstrates the process of training fraud detection models and tuning their hyperparameters. We'll work with both classification and autoencoder models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "\n",
    "# Add the src directory to the path to import our modules\n",
    "sys.path.append('..')\n",
    "from src.models.fraud_model import (\n",
    "    create_classification_model, \n",
    "    train_classification_model,\n",
    "    create_autoencoder_model,\n",
    "    train_autoencoder_model,\n",
    "    compute_anomaly_scores\n",
    ")\n",
    "from src.utils.data_utils import handle_class_imbalance\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data\n",
    "\n",
    "First, we'll load the processed data that was created by our PySpark preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Path to processed data\n",
    "processed_data_path = '../data/processed/transactions.parquet'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(processed_data_path):\n",
    "    # Load the data\n",
    "    df = pd.read_parquet(processed_data_path)\n",
    "    print(f\"Dataset loaded with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "else:\n",
    "    print(f\"File not found: {processed_data_path}\")\n",
    "    print(\"Please run the data processing script first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "Let's prepare the data for modeling by splitting it into training, validation, and test sets, and scaling the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Separate features and target\n",
    "if 'is_fraud' in df.columns:\n",
    "    y = df['is_fraud'].values\n",
    "    X = df.drop(columns=['is_fraud']).values\n",
    "    \n",
    "    # Check class distribution\n",
    "    print(\"Class distribution:\")\n",
    "    print(f\"Legitimate transactions: {np.sum(y == 0)} ({np.mean(y == 0)*100:.2f}%)\")\n",
    "    print(f\"Fraudulent transactions: {np.sum(y == 1)} ({np.mean(y == 1)*100:.2f}%)\")\n",
    "else:\n",
    "    print(\"Target column 'is_fraud' not found in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split data into train, validation, and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features scaled to zero mean and unit variance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save test data for later evaluation\n",
    "test_df = pd.DataFrame(X_test_scaled, columns=df.drop(columns=['is_fraud']).columns)\n",
    "test_df['is_fraud'] = y_test\n",
    "test_df.to_parquet('../data/processed/test_data.parquet')\n",
    "print(\"Test data saved for later evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Handle Class Imbalance\n",
    "\n",
    "Fraud detection datasets are typically highly imbalanced. Let's handle this imbalance for better model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check class imbalance\n",
    "print(\"Class distribution in training set:\")\n",
    "print(f\"Legitimate transactions: {np.sum(y_train == 0)} ({np.mean(y_train == 0)*100:.2f}%)\")\n",
    "print(f\"Fraudulent transactions: {np.sum(y_train == 1)} ({np.mean(y_train == 1)*100:.2f}%)\")\n",
    "\n",
    "# Handle class imbalance using SMOTE\n",
    "X_train_balanced, y_train_balanced = handle_class_imbalance(\n",
    "    X_train_scaled, y_train, method='smote', ratio=0.5\n",
    ")\n",
    "\n",
    "print(\"\\nClass distribution after balancing:\")\n",
    "print(f\"Legitimate transactions: {np.sum(y_train_balanced == 0)} ({np.mean(y_train_balanced == 0)*100:.2f}%)\")\n",
    "print(f\"Fraudulent transactions: {np.sum(y_train_balanced == 1)} ({np.mean(y_train_balanced == 1)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classification Model Training\n",
    "\n",
    "Let's train a classification model for fraud detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set up MLflow\n",
    "mlflow.set_experiment(\"Fraud_Detection_Experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define model parameters\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "hidden_layers = [128, 64, 32]\n",
    "dropout_rate = 0.4\n",
    "batch_size = 256\n",
    "epochs = 20\n",
    "\n",
    "# Create model directory if it doesn't exist\n",
    "model_dir = '../results/models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Train model with MLflow tracking\n",
    "with mlflow.start_run(run_name='classification_model_notebook'):\n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        'model_type': 'classification',\n",
    "        'hidden_layers': str(hidden_layers),\n",
    "        'dropout_rate': dropout_rate,\n",
    "        'batch_size': batch_size,\n",
    "        'epochs': epochs,\n",
    "        'input_dim': input_dim,\n",
    "        'balancing_method': 'smote',\n",
    "        'balancing_ratio': 0.5\n",
    "    })\n",
    "    \n",
    "    # Train model\n",
    "    model_path = os.path.join(model_dir, \"classification_model.h5\")\n",
    "    model, history = train_classification_model(\n",
    "        X_train_balanced, y_train_balanced, X_val_scaled, y_val,\n",
    "        input_dim=input_dim,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        model_path=model_path\n",
    "    )\n",
    "    \n",
    "    # Log metrics\n",
    "    for epoch, metrics in enumerate(history.history.items()):\n",
    "        metric_name, values = metrics\n",
    "        for i, value in enumerate(values):\n",
    "            mlflow.log_metric(f\"train_{metric_name}\", value, step=i)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.tensorflow.log_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Autoencoder Model Training\n",
    "\n",
    "Now, let's train an autoencoder model for anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Filter training data to include only non-fraud transactions\n",
    "X_train_normal = X_train_scaled[y_train == 0]\n",
    "X_val_normal = X_val_scaled[y_val == 0]\n",
    "\n",
    "print(f\"Normal transactions for autoencoder training: {X_train_normal.shape[0]}\")\n",
    "print(f\"Normal transactions for validation: {X_val_normal.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define model parameters\n",
    "input_dim = X_train_normal.shape[1]\n",
    "hidden_layers = [64, 32]\n",
    "encoding_dim = 16\n",
    "batch_size = 256\n",
    "epochs = 30\n",
    "\n",
    "# Train autoencoder model with MLflow tracking\n",
    "with mlflow.start_run(run_name='autoencoder_model_notebook'):\n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        'model_type': 'autoencoder',\n",
    "        'hidden_layers': str(hidden_layers),\n",
    "        'encoding_dim': encoding_dim,\n",
    "        'batch_size': batch_size,\n",
    "        'epochs': epochs,\n",
    "        'input_dim': input_dim\n",
    "    })\n",
    "    \n",
    "    # Train model\n",
    "    model_path = os.path.join(model_dir, \"autoencoder_model.h5\")\n",
    "    autoencoder, history = train_autoencoder_model(\n",
    "        X_train_normal, X_val_normal,\n",
    "        input_dim=input_dim,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        model_path=model_path\n",
    "    )\n",
    "    \n",
    "    # Log metrics\n",
    "    for epoch, metrics in enumerate(history.history.items()):\n",
    "        metric_name, values = metrics\n",
    "        for i, value in enumerate(values):\n",
    "            mlflow.log_metric(f\"train_{metric_name}\", value, step=i)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.tensorflow.log_model(autoencoder, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Autoencoder Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning\n",
    "\n",
    "Let's perform hyperparameter tuning to find the best model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define hyperparameter search space for classification model\n",
    "hidden_layers_options = [\n",
    "    [64, 32],\n",
    "    [128, 64, 32],\n",
    "    [256, 128, 64, 32]\n",
    "]\n",
    "dropout_rates = [0.3, 0.4, 0.5]\n",
    "learning_rates = [0.001, 0.0005, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Perform grid search for classification model\n",
    "best_val_auc = 0\n",
    "best_params = {}\n",
    "\n",
    "for hidden_layers in hidden_layers_options:\n",
    "    for dropout_rate in dropout_rates:\n",
    "        for lr in learning_rates:\n",
    "            print(f\"\\nTraining with: hidden_layers={hidden_layers}, dropout_rate={dropout_rate}, learning_rate={lr}\")\n",
    "            \n",
    "            with mlflow.start_run(run_name=f\"tuning_classification_hl{len(hidden_layers)}_dr{dropout_rate}_lr{lr}\"):\n",
    "                # Log parameters\n",
    "                mlflow.log_params({\n",
    "                    'model_type': 'classification',\n",
    "                    'hidden_layers': str(hidden_layers),\n",
    "                    'dropout_rate': dropout_rate,\n",
    "                    'learning_rate': lr,\n",
    "                    'batch_size': batch_size,\n",
    "                    'epochs': 10  # Reduced epochs for faster tuning\n",
    "                })\n",
    "                \n",
    "                # Create model\n",
    "                model = create_classification_model(input_dim, hidden_layers, dropout_rate)\n",
    "                \n",
    "                # Customize optimizer with learning rate\n",
    "                model.optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "                \n",
    "                # Train model\n",
    "                history = model.fit(\n",
    "                    X_train_balanced, y_train_balanced,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=10,  # Reduced epochs for faster tuning\n",
    "                    validation_data=(X_val_scaled, y_val),\n",
    "                    verbose=0\n",
    "                )\n",
    "                \n",
    "                # Evaluate on validation set\n",
    "                y_val_pred = model.predict(X_val_scaled)\n",
    "                val_auc = tf.keras.metrics.AUC()(y_val, y_val_pred).numpy()\n",
    "                \n",
    "                # Log metrics\n",
    "                mlflow.log_metric(\"val_auc\", val_auc)\n",
    "                \n",
    "                print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "                \n",
    "                # Update best parameters if better\n",
    "                if val_auc > best_val_auc:\n",
    "                    best_val_auc = val_auc\n",
    "                    best_params = {\n",
    "                        'hidden_layers': hidden_layers,\n",
    "                        'dropout_rate': dropout_rate,\n",
    "                        'learning_rate': lr\n",
    "                    }\n",
    "\n",
    "print(f\"\\nBest parameters: {best_params}\")\n",
    "print(f\"Best validation AUC: {best_val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Final Models with Best Parameters\n",
    "\n",
    "Now that we've found the best hyperparameters, let's train the final models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train final classification model with best parameters\n",
    "with mlflow.start_run(run_name='final_classification_model'):\n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        'model_type': 'classification',\n",
    "        'hidden_layers': str(best_params['hidden_layers']),\n",
    "        'dropout_rate': best_params['dropout_rate'],\n",
    "        'learning_rate': best_params['learning_rate'],\n",
    "        'batch_size': batch_size,\n",
    "        'epochs': epochs,\n",
    "        'final_model': True\n",
    "    })\n",
    "    \n",
    "    # Create model\n",
    "    final_model = create_classification_model(\n",
    "        input_dim, \n",
    "        best_params['hidden_layers'], \n",
    "        best_params['dropout_rate']\n",
    "    )\n",
    "    \n",
    "    # Customize optimizer with best learning rate\n",
    "    final_model.optimizer = tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'])\n",
    "    \n",
    "    # Train model\n",
    "    model_path = os.path.join(model_dir, \"final_classification_model.h5\")\n",
    "    history = final_model.fit(\n",
    "        X_train_balanced, y_train_balanced,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(X_val_scaled, y_val),\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5),\n",
    "            tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Log metrics\n",
    "    for epoch, metrics in enumerate(history.history.items()):\n",
    "        metric_name, values = metrics\n",
    "        for i, value in enumerate(values):\n",
    "            mlflow.log_metric(f\"train_{metric_name}\", value, step=i)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.tensorflow.log_model(final_model, \"model\")\n",
    "    \n",
    "    print(f\"Final classification model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "In this notebook, we've:\n",
    "1. Loaded and prepared the processed transaction data\n",
    "2. Handled class imbalance using SMOTE\n",
    "3. Trained classification and autoencoder models for fraud detection\n",
    "4. Performed hyperparameter tuning to find the best model configuration\n",
    "5. Trained final models with the best parameters\n",
    "\n",
    "The trained models are saved in the `results/models` directory and can be used for evaluation and deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
